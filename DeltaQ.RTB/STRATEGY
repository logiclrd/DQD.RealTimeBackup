Backup strategy:
* Handle file update procedure:
  => Poll every 2 seconds as long as there are open file handles
     * When there are no open file handles, create/locate a ZFS snapshot and then run Submit file procedure.
     * If the file is still open after 30 seconds, switch to a new polling method:
       1. Create/locate a ZFS snapshot
       2. If the file has no open file handles, then run Submit file procedure and exit.
       3. Wait 30 seconds
       4. Create/locate a ZFS snapshot
       5. Compare the file in the old and new snapshots.
       6. Drop the old snapshot, the new snapshot becomes the new old snapshot.
       7. If the snapshots are identical or the retry limit has been reached, then run Submit file procedure and exit, otherwise go to step 2.
* Submit file procedure:
  => Given a ZFS snapshot
  => If the file is small (say, <100 MB), copy it to /tmp then delete the snapshot, and upload it from /tmp
  => If the file is large, upload it from the snapshot then delete the snapshot
  => Before actually doing the upload, check the file's hash and skip it if it is unchanged from the last upload
     * Assume that no other sources are tampering with the files, so we can simply maintain a local cache of uploaded files with their checksums at the time
* ZFS snapshots can be shared if they were taken within the same snapshot sharing interval => timestamp the creation, reuse if the age is < AgeLimit
  => Reference counter will be needed for ZFS snapshots to ensure that they aren't deleted until no processes are using them
* Restrict number of concurrent tasks (separately for small vs. large files)

* Periodically scan all files to check that their hashes match
  * Ignore files whose size and last-modified date are unchanged
  * Delay before the next scan equal to length of current scan, but no less than 2 hours and no more than 24 hours

How to use B2 API (I think):
* Authenticate
* Call b2_get_upload_url
* Upload multiple files to the returned URL, as needed
* If uploading files in parallel, obtain a different upload URL for each thread
* Renaming files is not supported, so instead each time a new file is started, it needs to be assigned a unique ID. If the client side detects a rename, it remaps the ID. Changes to the ID mappings need to be uploaded. Should probably do this in incremental chunks, and consolidate them periodically. The current state is always the result of merging all current state files, in order, so making a new file with the entire current state and then deleting all the previous files will not affect the mappings. Do this once a month or something.
